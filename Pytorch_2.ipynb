{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40df8f4d",
   "metadata": {},
   "source": [
    "## 张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b77fb575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import Compose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3403d4",
   "metadata": {},
   "source": [
    "nn.Linear(in_features=10,out_features=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1336ec15",
   "metadata": {},
   "source": [
    "### 张量的数据类型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5975fbab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aaf92e06",
   "metadata": {},
   "source": [
    "如何查看和设置张量的数据类型: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f7a2d39b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "## 获取张量的数据类型\n",
    "torch.tensor([1.2, 3.4]).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "83aa63b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 将张量的默认数据类型设置为其他类型\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "torch.tensor([1.2, 3.4]).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "eac47de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.dtype: torch.float64\n",
      "a.long()方法: torch.int64\n",
      "a.int()method: torch.int32\n",
      "a.float()method: torch.float32\n"
     ]
    }
   ],
   "source": [
    "## 将张量数据类型转化为整形\n",
    "a = torch.tensor([1.2, 3.4])\n",
    "print(\"a.dtype:\",a.dtype)\n",
    "print(\"a.long()方法:\", a.long().dtype)\n",
    "print(\"a.int()method:\", a.int().dtype)\n",
    "print(\"a.float()method:\", a.float().dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b8a9d531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## restore the default data type of torch\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "torch.tensor([1.2, 3.4]).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "84c6d5f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## another way to get the default data type\n",
    "torch.get_default_dtype()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd30e57",
   "metadata": {},
   "source": [
    "### 张量的生成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c02a3b",
   "metadata": {},
   "source": [
    "（1）使用torch.tensor()函数生成张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f71e7d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [2., 2.]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1.0,1.0],[2,2]])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7b7372d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e192c66",
   "metadata": {},
   "source": [
    "上面的程序使用torch.tensor()函数将python的列表转化为张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d5d15fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 获取张量的维度\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3530c6f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 获取张量的形状大小\n",
    "A.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "23c72abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 计算张量中包含元素的数量\n",
    "A.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bddafb5",
   "metadata": {},
   "source": [
    "在使用torch.tensor()函数时，可以使用参数dtype来指定张量的数据类型，使用参数requires_grad来指定张量是否需要计算梯度。\n",
    "只有就散了梯度的向量才能在深度深度网络优化时根据梯度大小进行更新。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0f729143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 生成一个需要计算梯度的张量B\n",
    "B = torch.tensor((1,2,3),dtype=torch.float32,requires_grad=True)\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ff4fbd05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 6.])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 因为张量B是可计算梯度的，故可以计算sum(B**2)的梯度\n",
    "y = B.pow(2).sum()\n",
    "y.backward()\n",
    "B.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f307b91",
   "metadata": {},
   "source": [
    "从输出结果可以看出每个位置上的梯度为2×B。需要注意的是，只有浮点型数据才能计算梯度，其他类型的数据不能计算张量的梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405c3086",
   "metadata": {},
   "source": [
    "（2）torch.Tensor()函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c7ff5a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 根据已有数据创建张量\n",
    "C = torch.Tensor([1,2,3,4])\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f6872439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 1.8750, 0.0000],\n",
       "        [1.8750, 0.0000, 2.0000]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 创建具有特定大小的张量\n",
    "D = torch.Tensor(2,3)\n",
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4647956a",
   "metadata": {},
   "source": [
    "针对已经生成的张量可以使用torch.**like()系列函数生成与指定张量维度相同、性质相似的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b8584d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 创建一个与D相同大小和类型的全1张量\n",
    "torch.ones_like(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f9e52be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 生成与D维度相同的全0张量\n",
    "torch.zeros_like(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ab00b81d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3821, 0.6605, 0.8536],\n",
       "        [0.5932, 0.6367, 0.9826]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 生成与D维度相同的随机张量\n",
    "torch.rand_like(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d6f1c7",
   "metadata": {},
   "source": [
    "针对一个创建好的张量D，可以使用D.new_**()系列函数创建出新的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "86d1ce2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D.dtype : torch.float32\n",
      "E.dtype : torch.float32\n"
     ]
    }
   ],
   "source": [
    "## 创建一个类型相似但尺寸不同的张量\n",
    "## 将列表E转化为32位浮点型的张量\n",
    "E = [[1,2],[3,3]]\n",
    "E = D.new_tensor(E)\n",
    "print(\"D.dtype :\",D.dtype)\n",
    "print(\"E.dtype :\",E.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd35c0ac",
   "metadata": {},
   "source": [
    "（3）张量和NumPy数据相互转换\n",
    "PyTorch提供了Numpy数组和PyTorch张量相互转换的函数，非常方便对张量进行相关操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e6a96e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 利用Numpy数组生成张量\n",
    "import numpy as np\n",
    "F = np.ones((3,3))\n",
    "\n",
    "## 使用torch.as_tensor()函数\n",
    "Ftensor = torch.as_tensor(F)\n",
    "Ftensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d785d925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 使用torch.from_numpy()函数\n",
    "Ftensor = torch.from_numpy(F)\n",
    "Ftensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4abb338",
   "metadata": {},
   "source": [
    "上面得到的张量是64位浮点型数据，这是因为使用Numpy生成的数组默认是64位浮点型数组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "dc8ab3ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 使用张量的.numpy()将张量转化为Numpy数组\n",
    "Ftensor.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2521ac2",
   "metadata": {},
   "source": [
    "(4)随机数生成向量\n",
    "在PyTorch中还可以通过相关随机数来生成张量。在生成随机数前，可以使用torch.manual_seed()函数，指定生成随机数的种子，用于保证生成的随机数是可重复出现的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f42fdc62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1115)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 通过指定均值和标准差生成随机数\n",
    "## 通过mean指定随机数的均值，std指定随机数的标准差\n",
    "torch.manual_seed(123)\n",
    "A = torch.normal(mean = 0.0,std = torch.tensor(1.0))\n",
    "A"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d84e335f",
   "metadata": {},
   "source": [
    "如果mean和std参数有多个值，则可生成多个随机数，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "bae6adfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1115,  0.2407, -1.1089, -0.9617])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 每个随机数服从的分布均值是0，但是他们分布的标准差分别为1、2、3、4\n",
    "torch.manual_seed(123)\n",
    "A = torch.normal(mean = 0.0,std=torch.arange(1,5.0))\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "880e4eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8885, 2.2407, 1.8911, 3.0383])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 分别制定每个随机数服从的均值和标准差\n",
    "torch.manual_seed(123)\n",
    "A = torch.normal(mean = torch.arange(1,5.0),std=torch.arange(1,5.0))\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d1bcd8",
   "metadata": {},
   "source": [
    "也可以使用torch.rand()函数，在区间[0,1]上服从均匀分布的张量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1805a650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2961, 0.5166, 0.2517, 0.6886],\n",
       "        [0.0740, 0.8665, 0.1366, 0.1025],\n",
       "        [0.1841, 0.7264, 0.3153, 0.6871]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "B = torch.rand(3,4)\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169cd972",
   "metadata": {},
   "source": [
    "而torch.rand_like()函数则可以生成与其他张量维度相同的随机数张量，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "10864273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2961, 0.5166, 0.2517],\n",
       "        [0.6886, 0.0740, 0.8665]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "C = torch.ones(2,3)\n",
    "D = torch.rand_like(C)\n",
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ae62fa",
   "metadata": {},
   "source": [
    "使用torch.randn()和torch.rand_like()函数则可生成服从标准正态分布的随机数张量，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "dafb46f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9447,  0.6217, -1.3501],\n",
      "        [-0.1881, -2.3891, -0.4759],\n",
      "        [ 1.7603,  0.6547,  0.5490]])\n",
      "tensor([[ 0.3671,  0.1219,  0.6466],\n",
      "        [-1.4168,  0.8429, -0.6307]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.randn(3,3))\n",
    "print(torch.randn_like(C))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347b5a04",
   "metadata": {},
   "source": [
    "使用torch.randperm(n)函数，则可将0-n（包含0不包含n）之间的整数进行随机排序后输出，例如将0-9这10个数字随机排序后输出："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d33a8ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 0, 8, 1, 3, 7, 4, 9, 5, 6])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "torch.randperm(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f6981d",
   "metadata": {},
   "source": [
    "(5)其他生成张量的函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778211f4",
   "metadata": {},
   "source": [
    "在PyTorch中包含和np.arange()用法相似的函数torch.arange(),常用来生成张量，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2cc36359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 4, 6, 8])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(start=0, end = 10, step=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3051c523",
   "metadata": {},
   "source": [
    "可使用torch.linspace()函数在范围内生成固定数量的等间隔张量，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d1d14244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000,  3.2500,  5.5000,  7.7500, 10.0000])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(start = 1, end = 10, steps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937748de",
   "metadata": {},
   "source": [
    "torch.logspace()函数则可生成对数为间隔的张量，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5f9c184d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2589,  2.1135,  3.5481,  5.9566, 10.0000])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.logspace(start = 0.1, end=1.0, steps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c8fbea",
   "metadata": {},
   "source": [
    "常用的生成张量系列函数："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978db85b",
   "metadata": {},
   "source": [
    "| 函数                              | 描述                  |\n",
    "| --------------------------------- | --------------------- |\n",
    "| torch.zeros(3,3)                  | 3×3的全0张量          |\n",
    "| torch.ones(3,3)                   | 3×3的全1张量          |\n",
    "| torch.full((3,3),fill_value=0.25) | 3×3使用0.25填充的张量 |\n",
    "| torch.empty(3,3)                  | 3×3的空张量           |\n",
    "| torch.eye(3)                      | 3×3的单位张量         |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39936f4",
   "metadata": {},
   "source": [
    "### 张量操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f361c229",
   "metadata": {},
   "source": [
    "（1）张量的形状 \n",
    "\n",
    "改变张量的形状在深度学习的使用过程中会经常遇到，而且针对不同的情况对张量的形状尺寸的改变有多种函数和方法可以使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3e83d36a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 使用tensor.reshape()方法设置张量的形状大小\n",
    "A = torch.arange(12.0).reshape(3,4)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b08310",
   "metadata": {},
   "source": [
    "上面的程序针对一个生成的张量，利用.reshape()方法修改张量。除此之外还可以直接通过torch.reshape()修改输入张量的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "df419001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
       "        [ 6.,  7.,  8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 使用torch.reshape()\n",
    "torch.reshape(input = A,shape = (2,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eb1ee9",
   "metadata": {},
   "source": [
    "改变张量的形状使用tensor.resize_()方法，针对输入的形状大小对张量形状进行修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "95cc0ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
       "        [ 6.,  7.,  8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## using resize_()\n",
    "A.resize_(2,6)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced3b2cd",
   "metadata": {},
   "source": [
    "在PyTorch中提供了A.resize_as_(B)方法，可以将张量A的形状尺寸修改为与B相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "803554f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [3., 4., 5.],\n",
       "        [6., 7., 8.]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.arange(10.0,19.0).reshape(3,3)\n",
    "A.resize_as_(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c9505cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., 11., 12.],\n",
       "        [13., 14., 15.],\n",
       "        [16., 17., 18.]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae330628",
   "metadata": {},
   "source": [
    "在PyTorch中，torch.unsqueeze()函数可以在张量的指定维度插入新的维度得到维度提升的张量，而torch.squeeze()函数，可以移除指定或者所有维度大小为1的维度，从而得到维度减小的新张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a4006e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 6])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## torch.unsqueeze()函数在指定维度插入尺寸为1的新张量\n",
    "A = torch.arange(12.0).reshape(2,6)\n",
    "B = torch.unsqueeze(A,dim = 0)\n",
    "B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4232ace6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C.shape :  torch.Size([1, 2, 6, 1])\n",
      "D.shape :  torch.Size([2, 6])\n",
      "E.shape :  torch.Size([2, 6, 1])\n"
     ]
    }
   ],
   "source": [
    "## torch.squeeze()函数移除所有维度为1的维度\n",
    "C = B.unsqueeze(dim = 3)\n",
    "print(\"C.shape : \",C.shape)\n",
    "D = torch.squeeze(C)\n",
    "print(\"D.shape : \",D.shape)\n",
    "\n",
    "## 移除指定维度为1的维度\n",
    "E = torch.squeeze(C,dim = 0)\n",
    "print(\"E.shape : \",E.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187d1270",
   "metadata": {},
   "source": [
    "在PyTorch中也可以使用.expand()方法对张量的维度进行拓展，从而修改张量的形状大小。\n",
    "而A.expand_as(C)方法则会将张量A根据张量C的形状大小进行拓展，得到新的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3841a990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [0, 1, 2],\n",
       "        [0, 1, 2]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## using .expand() to expand tensor\n",
    "A = torch.arange(3)\n",
    "B = A.expand(3,-1)\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8669e081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [0, 1, 2]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## using .expand_as() to expand tensor\n",
    "C = torch.arange(6).reshape(2,3)\n",
    "B = A.expand_as(C)\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d0e1e",
   "metadata": {},
   "source": [
    "使用张量的.repeat()方法可以讲张量看做一个整体，然后根据制定的形状进行重复填充，得到新的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "047cd3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 1, 2, 0, 1, 2],\n",
      "         [0, 1, 2, 0, 1, 2],\n",
      "         [0, 1, 2, 0, 1, 2],\n",
      "         [0, 1, 2, 0, 1, 2]]])\n",
      "torch.Size([2, 3])\n",
      "torch.Size([1, 4, 6])\n"
     ]
    }
   ],
   "source": [
    "## usign .repeat() to expand tensor\n",
    "D = B.repeat(1,2,2)\n",
    "print(D)\n",
    "print(B.shape)\n",
    "print(D.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1574cf20",
   "metadata": {},
   "source": [
    "（2）获取张量中的元素\n",
    "从已知的张量总提取需要的元素，在实际应用中也很常见。从张量中利用切片和索引提取元素的方法，和在Numpy的使用方法是一致的，所以使用时非常方便。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "44f41737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get elements in the tensor using slice and index\n",
    "A = torch.arange(12).reshape(1,3,4)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "edda82e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "baa990d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3],\n",
       "        [4, 5, 6, 7]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 获取第0维度下的矩阵前两行元素\n",
    "A[0,0:2,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8b16d0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8,  9, 10])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 获取第0维度下的矩阵，最后一行-4 ~ -1列\n",
    "A[0,-1,-4:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1412925f",
   "metadata": {},
   "source": [
    "也可以按需将索引设置为相应的布尔值，然后提取真条件下的内容，例如找到A中取指大于5的元素："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "068529f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0, -1, -2, -3],\n",
       "         [-4, -5,  6,  7],\n",
       "         [ 8,  9, 10, 11]]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 根据条件筛选\n",
    "B = -A\n",
    "torch.where(A>5,A,B) ## 当A>5为真时返回A对应的值，为假时返回B对应值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a3c8b1bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 获取A中大于5的元素\n",
    "A[A > 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abba2e7",
   "metadata": {},
   "source": [
    "torch.tril()函数可以获取张量下三角部分的内容，而将上三角的元素设置为0;torch.triu()函数可以获取上三角部分的内容，而将下三角部分的元素设置为0；torch.diag()函数可以获取矩阵张量的对角线元素，或者体用一个向量生成一个矩阵张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0b6cbe22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  0,  0,  0],\n",
       "         [ 4,  5,  0,  0],\n",
       "         [ 8,  9, 10,  0]]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 获取矩阵张量的下三角部分\n",
    "torch.tril(A,diagonal=0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "2b562312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  0,  0],\n",
       "         [ 4,  5,  6,  0],\n",
       "         [ 8,  9, 10, 11]]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## diagonal参数控制考虑的对角线\n",
    "torch.tril(A,diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a1164176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 0,  5,  6,  7],\n",
       "         [ 0,  0, 10, 11]]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 获取矩阵张量的上三角部分\n",
    "torch.triu(A,diagonal=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d1f68973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([ 0,  5, 10])\n",
      "tensor([ 1,  6, 11])\n"
     ]
    }
   ],
   "source": [
    "## 获取矩阵张量的对角线元素，input需要是一个二维的张量\n",
    "C = A.reshape(3,4)\n",
    "print(C)\n",
    "print(torch.diag(C,diagonal=0))\n",
    "print(torch.diag(C,diagonal=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866e61e9",
   "metadata": {},
   "source": [
    "在上面的程序中可以通过diagoal参数来控制获取的对角线元素，相对于对角线的位移。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "78c540c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [0, 2, 0],\n",
       "        [0, 0, 3]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 提供对角线元素生成矩阵张量\n",
    "torch.diag(torch.tensor([1,2,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370b2fad",
   "metadata": {},
   "source": [
    "（3）拼接和拆分\n",
    "PyTorch还提供了将多个张量拼接为一个张量、将一个大的张量拆分为几个小的张量的函数。其中torch.cat()函数可以将多个张量在指定的维度进行拼接，得到新的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "50a6ff2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.],\n",
       "        [ 3.,  4.,  5.],\n",
       "        [ 0.,  2.,  4.],\n",
       "        [ 6.,  8., 10.]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 在给定维度中连接给定的张量序列\n",
    "A = torch.arange(6.0).reshape(2,3)\n",
    "B = torch.linspace(0,10,6).reshape(2,3)\n",
    "## 在0维度连接张量\n",
    "C = torch.cat((A,B),dim=0)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "633faffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  0.,  2.,  4.],\n",
       "        [ 3.,  4.,  5.,  6.,  8., 10.]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 在1维度连接向量\n",
    "D = torch.cat((A,B),dim=1)\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a84889fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  0.,  1.,  2.,  0.,  2.,  4.],\n",
       "        [ 4.,  3.,  4.,  5.,  6.,  8., 10.]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 在1维度连接3个张量\n",
    "E = torch.cat((A[:,1:2],A,B),dim=1)\n",
    "E"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e025c248",
   "metadata": {},
   "source": [
    "PyTorch中的torch.stack()函数也可以讲多个张量按照指定的维度进行拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "24a16266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  1.,  2.],\n",
      "         [ 3.,  4.,  5.]],\n",
      "\n",
      "        [[ 0.,  2.,  4.],\n",
      "         [ 6.,  8., 10.]]])\n",
      "torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "## 沿新维度连接向量\n",
    "F = torch.stack((A,B),dim=0)\n",
    "print(F)\n",
    "print(F.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "9390a7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.,  0.],\n",
      "         [ 1.,  2.],\n",
      "         [ 2.,  4.]],\n",
      "\n",
      "        [[ 3.,  6.],\n",
      "         [ 4.,  8.],\n",
      "         [ 5., 10.]]])\n",
      "torch.Size([2, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "G = torch.stack((A,B),dim=2)\n",
    "print(G)\n",
    "print(G.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a048c3",
   "metadata": {},
   "source": [
    "torch.chunk()函数可以将张量分割为特定数量的块；torch.split()函数在将张量分割为特定数量的块时，可以指定每个块的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "88126950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 1., 2.],\n",
       "         [4., 3., 4., 5.]]),\n",
       " tensor([[ 0.,  2.,  4.],\n",
       "         [ 6.,  8., 10.]]))"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 在行上将张量E分为两块\n",
    "torch.chunk(E,2,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "61f753a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]])\n",
      "tensor([[ 0.,  2.,  4.],\n",
      "        [ 6.,  8., 10.]])\n"
     ]
    }
   ],
   "source": [
    "D1,D2 = torch.chunk(D,2,dim=1)\n",
    "print(D1)\n",
    "print(D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d705049b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1.],\n",
      "        [4., 3., 4.]])\n",
      "tensor([[2., 0., 2.],\n",
      "        [5., 6., 8.]])\n",
      "tensor([[ 4.],\n",
      "        [10.]])\n"
     ]
    }
   ],
   "source": [
    "## 如果沿给定维度dim的张量大小不能被块整除，则最后一个块将最小\n",
    "E1,E2,E3 = torch.chunk(E,3,dim=1)\n",
    "print(E1)\n",
    "print(E2)\n",
    "print(E3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "a7274453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [3.]])\n",
      "tensor([[1., 2.],\n",
      "        [4., 5.]])\n",
      "tensor([[ 0.,  2.,  4.],\n",
      "        [ 6.,  8., 10.]])\n"
     ]
    }
   ],
   "source": [
    "## 将张量切分为块，指定每个块的大小\n",
    "D1,D2,D3 = torch.split(D,[1,2,3],dim=1)\n",
    "print(D1)\n",
    "print(D2)\n",
    "print(D3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26776a9",
   "metadata": {},
   "source": [
    "### 张量计算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7193ded6",
   "metadata": {},
   "source": [
    "针对张量计算的内容，主要包括张量之间的大小比较、张量的基本运算、张量与统计相关的运算。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe35b394",
   "metadata": {},
   "source": [
    "（1）比较大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "28abf44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "## 比较两个数是否接近\n",
    "A = torch.tensor([10.0])\n",
    "B = torch.tensor([10.1])\n",
    "print(torch.allclose(A,B, rtol=1e-05, atol=1e-08, equal_nan=False))\n",
    "print(torch.allclose(A,B, rtol=0.1, atol=0.01, equal_nan=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae474da",
   "metadata": {},
   "source": [
    "从输出结果可以发现，在不同的参数条件下会给出不同的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "b7ca76a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "## 如果equal_nan=True，那么缺失值可以判断接近\n",
    "A = torch.tensor(float(\"nan\"))\n",
    "print(torch.allclose(A,A, equal_nan=False))\n",
    "print(torch.allclose(A,A, equal_nan=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92c776a",
   "metadata": {},
   "source": [
    "torch.eq()函数用来判断两个元素是否相等；torch.equal()函数可以判断两个张量是否具有相同的形状和元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "64bf4138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True])\n",
      "tensor([[True, True, True, True, True, True]])\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "## 计算元素是否相等\n",
    "A = torch.tensor([1,2,3,4,5,6])\n",
    "B = torch.arange(1,7)\n",
    "C = torch.unsqueeze(B,dim=0)\n",
    "print(torch.eq(A,B))\n",
    "print(torch.eq(A,C))\n",
    "## 判断两个张量是否具有相同的形状和元素\n",
    "print(torch.equal(A,B))\n",
    "print(torch.equal(A,C))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea3f7f7",
   "metadata": {},
   "source": [
    "torch.ge()函数是逐元素比较是否大于等于；torch.gt()函数是逐元素比较大于"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ecf66d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True])\n",
      "tensor([[True, True, True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "## 逐元素比较大于等于\n",
    "print(torch.ge(A,B))\n",
    "print(torch.ge(A,C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "97d6f9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False, False, False, False, False, False])\n",
      "tensor([[False, False, False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "## 逐元素比较小于\n",
    "print(torch.gt(A,B))\n",
    "print(torch.gt(A,C))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eeae7d",
   "metadata": {},
   "source": [
    "torch.le()函数是逐元素比较是否小于等于；torch.lt()函数是逐元素比较小于"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4bcb03",
   "metadata": {},
   "source": [
    "torch.ne()函数是逐元素比较不等于；torch.isnan()函数用来判断是否为缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "dedda2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False,  True, False])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 判断是否为缺失值\n",
    "torch.isnan(torch.tensor([0,1,float(\"nan\"),2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11af1457",
   "metadata": {},
   "source": [
    "（2）基本运算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dadc1ca",
   "metadata": {},
   "source": [
    "张量的基本运算方式，一种为逐元素之间的运算，如加减乘除、幂运算、平方根、对数等；另一种为矩阵之间的运算，如矩阵相乘、矩阵的转置等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "98b325f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]])\n",
      "B: tensor([[10., 12., 14.],\n",
      "        [16., 18., 20.]])\n",
      "tensor([[  0.,  12.,  28.],\n",
      "        [ 48.,  72., 100.]])\n",
      "tensor([[0.0000, 0.0833, 0.1429],\n",
      "        [0.1875, 0.2222, 0.2500]])\n"
     ]
    }
   ],
   "source": [
    "## 矩阵逐元素相乘\n",
    "A =torch.arange(6.0).reshape(2,3)\n",
    "B = torch.linspace(10,20,steps=6).reshape(2,3)\n",
    "print(\"A:\",A)\n",
    "print(\"B:\",B)\n",
    "print(A * B)\n",
    "## 逐元素相除\n",
    "print(A / B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c15dc885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10., 13., 16.],\n",
      "        [19., 22., 25.]])\n",
      "tensor([[-10., -11., -12.],\n",
      "        [-13., -14., -15.]])\n",
      "tensor([[inf, 12.,  7.],\n",
      "        [ 5.,  4.,  4.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0b/gzxsb2qj1570ktpd81lr27zw0000gn/T/ipykernel_22276/1494973912.py:6: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  print(B//A)\n"
     ]
    }
   ],
   "source": [
    "## 逐元素相加\n",
    "print(A + B)\n",
    "## 逐元素相减\n",
    "print(A - B)\n",
    "## 逐元素整除\n",
    "print(B//A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "9ca34dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0.,   1.,   8.],\n",
      "        [ 27.,  64., 125.]])\n",
      "tensor([[  0.,   1.,   8.],\n",
      "        [ 27.,  64., 125.]])\n"
     ]
    }
   ],
   "source": [
    "## 计算张量的幂可以使用torch.pow()函数，或者**运算符号\n",
    "print(torch.pow(A,3))\n",
    "print(A ** 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022be78c",
   "metadata": {},
   "source": [
    "计算张量的指数可以使用torch.exp()函数，计算张量的对数可以使用torch.log()函数，计算平方根可以使用torch.sqrt()函数，计算张量的平方根倒数可以使用torch.rsqrt()函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e07deba",
   "metadata": {},
   "source": [
    "针对张量数据的裁剪，有根据最大值裁剪torch,clamp_max()，有根据最小值裁剪torch.clamp_min()，还有根据范围裁剪torch.clamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "27a1283b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [3., 4., 4.]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 根据最大值裁剪\n",
    "torch.clamp_max(A,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "7cd9a977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3., 3.],\n",
       "        [3., 4., 5.]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 根据最小值裁剪\n",
    "torch.clamp_min(A,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "ce696bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 2.],\n",
       "        [3., 4., 5.]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 根据范围裁剪\n",
    "torch.clamp(A,2,5.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4767499d",
   "metadata": {},
   "source": [
    "前面介绍的都是张量中逐元素进行计算的方式，对于张量矩阵的运算函数如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "5a97e7d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 3.],\n",
       "        [1., 4.],\n",
       "        [2., 5.]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 矩阵的转置\n",
    "C = torch.t(A)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "da95d9db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5., 14.],\n",
       "        [14., 50.]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 矩阵运算，矩阵相乘。A的行数要等于C的列数\n",
    "A.matmul(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "825a38aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 10.,  13.],\n",
       "         [ 28.,  40.]],\n",
       "\n",
       "        [[172., 193.],\n",
       "         [244., 274.]]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(12.0).reshape(2,2,3)\n",
    "B = torch.arange(12.0).reshape(2,3,2)\n",
    "AB = torch.matmul(A,B)\n",
    "AB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "73f67814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True],\n",
      "        [True, True]])\n",
      "tensor([[True, True],\n",
      "        [True, True]])\n"
     ]
    }
   ],
   "source": [
    "## 矩阵相乘只计算最后面的两个维度的惩罚\n",
    "print(AB[0].eq(torch.matmul(A[0],B[0])))\n",
    "print(AB[1].eq(torch.matmul(A[1],B[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce5d213",
   "metadata": {},
   "source": [
    "若A和B互为逆矩阵，计算矩阵的逆矩阵使用torch.inverse()函数；一个方阵中，对角线元素的和称为矩阵的迹，可以使用torch.trace()计算得到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "412a144f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00,  2.9802e-08, -1.1921e-07],\n",
       "        [ 2.9802e-08,  1.0000e+00, -5.9605e-08],\n",
       "        [ 0.0000e+00,  5.9605e-08,  1.0000e+00]])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 计算矩阵的逆\n",
    "C = torch.rand(3,3)\n",
    "D = torch.inverse(C)\n",
    "torch.mm(C,D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a0c986a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 计算矩阵的迹\n",
    "torch.trace(torch.arange(9.0).reshape(3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c12c404",
   "metadata": {},
   "source": [
    "（3）统计相关的计算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dea7f77",
   "metadata": {},
   "source": [
    "     pytorch中包含了一些基础的统计计算功能，可以方便地获取张量中的均值、标准差、最大值、最小值及位置等。\n",
    "     torch.max()可以张量中的最大值\n",
    "     torch.argmax()输出最大值所在的位置\n",
    "     torch.argmin()输出最小值所在的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "185fd9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max num: tensor(99.)\n",
      "max num position: tensor(8)\n"
     ]
    }
   ],
   "source": [
    "## 一维张量的最大值\n",
    "A = torch.tensor([12.,34,25,11,67,32,29,30,99,55,23,44])\n",
    "## 最大值及位置\n",
    "print(\"max num:\",A.max())\n",
    "print(\"max num position:\",A.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "77c52795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max num: torch.return_types.max(\n",
      "values=tensor([34., 67., 99.]),\n",
      "indices=tensor([1, 0, 0]))\n",
      "max num postion tensor([1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "## 二维张量的最大值\n",
    "B = A.reshape(3,4)\n",
    "## 最大值及位置（每行）\n",
    "print(\"max num:\",B.max(dim=1))\n",
    "print(\"max num postion\",B.argmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2592ae87",
   "metadata": {},
   "source": [
    "torch.sort()可以对一维张量进行排序，或者对高维张量在指定的维度进行排序，在输出排序结果的同时，还会输出对应的值在原始位置的索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "710c9478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([11., 12., 23., 25., 29., 30., 32., 34., 44., 55., 67., 99.]),\n",
       "indices=tensor([ 3,  0, 10,  2,  6,  7,  5,  1, 11,  9,  4,  8]))"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 张量排序，分别输出从小到大的排序结果和相应元素在原始位置的索引\n",
    "torch.sort(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "e329cda1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([99., 67., 55., 44., 34., 32., 30., 29., 25., 23., 12., 11.]),\n",
       "indices=tensor([ 8,  4,  9, 11,  1,  5,  7,  6,  2, 10,  0,  3]))"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 按照降序排列\n",
    "torch.sort(A,descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "2753c1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B sort:\n",
      " tensor([[11., 12., 25., 34.],\n",
      "        [29., 30., 32., 67.],\n",
      "        [23., 44., 55., 99.]])\n",
      "B sort index:\n",
      " tensor([[3, 0, 2, 1],\n",
      "        [2, 3, 1, 0],\n",
      "        [2, 3, 1, 0]])\n",
      "B argsort:\n",
      " tensor([[3, 0, 2, 1],\n",
      "        [2, 3, 1, 0],\n",
      "        [2, 3, 1, 0]])\n"
     ]
    }
   ],
   "source": [
    "## 对2-D张量进行排序\n",
    "Bsort, Bsort_id=torch.sort(B)\n",
    "print(\"B sort:\\n\",Bsort)\n",
    "print(\"B sort index:\\n\",Bsort_id)\n",
    "print(\"B argsort:\\n\",torch.argsort(B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf7e5d2",
   "metadata": {},
   "source": [
    "torch.topk()根据指定的k值，计算张量中取指大小为前k大的数值与数值所在的位置。\n",
    "torch,kthvalue()根据指定的k值，计算张量大小为第k小的数值与数值所在的位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "6bb73ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([99., 67., 55.]),\n",
       "indices=tensor([8, 4, 9]))"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 获取张量前几个大的数值\n",
    "torch.topk(A,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "fcc49a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[34., 25.],\n",
      "        [67., 32.],\n",
      "        [99., 55.]])\n",
      "tensor([[1, 2],\n",
      "        [0, 1],\n",
      "        [0, 1]])\n"
     ]
    }
   ],
   "source": [
    "## 获取2-D张量每列前几个大的数值\n",
    "Btop2,Btop2_id = torch.topk(B,2,dim=1)\n",
    "print(Btop2)\n",
    "print(Btop2_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "51bf6e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.kthvalue(\n",
       "values=tensor(23.),\n",
       "indices=tensor(10))"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 获取张量第k小的数值和位置\n",
    "torch.kthvalue(A,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "78a6d97a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.kthvalue(\n",
       "values=tensor([12., 30., 44.]),\n",
       "indices=tensor([0, 3, 3]))"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 获取2-D张量第k小的数值和位置\n",
    "torch.kthvalue(B,2,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "e880138c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12.],\n",
       "        [30.],\n",
       "        [44.]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bkth,Bkth_id = torch.kthvalue(B,2,dim = 1,keepdim=True)\n",
    "Bkth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e4a16d",
   "metadata": {},
   "source": [
    "torch.mean()根据指定的维度计算均值\n",
    "torch.sum()根据指定的维度求和\n",
    "torch.cumsum()根据指定的维度计算累加和\n",
    "torch.median()根据指定的维度计算中位数\n",
    "torch.cumprod()根据指定的维度计算累乘积\n",
    "torch.std()计算张量的标准差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "2adae6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[20.5000],\n",
      "        [39.5000],\n",
      "        [55.2500]])\n",
      "tensor([[59.3333, 40.3333, 25.6667, 28.3333]])\n"
     ]
    }
   ],
   "source": [
    "## 计算每行的平均值\n",
    "print(torch.mean(B,dim = 1,keepdim = True))\n",
    "## 计算每列的平均值\n",
    "print(torch.mean(B,dim = 0,keepdim = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "03981fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 82.],\n",
      "        [158.],\n",
      "        [221.]])\n"
     ]
    }
   ],
   "source": [
    "## 计算每行的和\n",
    "print(torch.sum(B,dim=1,keepdim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "076eeb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[178., 121.,  77.,  85.]])\n"
     ]
    }
   ],
   "source": [
    "## 计算每列的和\n",
    "print(torch.sum(B,dim=0,keepdim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "c1017341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 12.,  46.,  71.,  82.],\n",
      "        [ 67.,  99., 128., 158.],\n",
      "        [ 99., 154., 177., 221.]])\n"
     ]
    }
   ],
   "source": [
    "## 按照行计算累加和\n",
    "print(torch.cumsum(B,dim = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e334d27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 12.,  34.,  25.,  11.],\n",
      "        [ 79.,  66.,  54.,  41.],\n",
      "        [178., 121.,  77.,  85.]])\n"
     ]
    }
   ],
   "source": [
    "## 按照列计算累加和\n",
    "print(torch.cumsum(B,dim = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "3eb8bd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.median(\n",
      "values=tensor([[12.],\n",
      "        [30.],\n",
      "        [44.]]),\n",
      "indices=tensor([[0],\n",
      "        [3],\n",
      "        [3]]))\n",
      "torch.return_types.median(\n",
      "values=tensor([[67., 34., 25., 30.]]),\n",
      "indices=tensor([[1, 0, 0, 1]]))\n"
     ]
    }
   ],
   "source": [
    "## 计算每行的中位数\n",
    "print(torch.median(B,dim = 1,keepdim = True))\n",
    "## 计算每列的中位数\n",
    "print(torch.median(B,dim = 0,keepdim = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "7060b46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 112200.],\n",
      "        [1865280.],\n",
      "        [5510340.]])\n",
      "tensor([[79596., 59840., 16675., 14520.]])\n"
     ]
    }
   ],
   "source": [
    "## 按照行计算乘积\n",
    "print(torch.prod(B,dim = 1,keepdim=True))\n",
    "## 按照列计算乘积\n",
    "print(torch.prod(B,dim = 0,keepdim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "c8ea4450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2000e+01, 4.0800e+02, 1.0200e+04, 1.1220e+05],\n",
      "        [6.7000e+01, 2.1440e+03, 6.2176e+04, 1.8653e+06],\n",
      "        [9.9000e+01, 5.4450e+03, 1.2524e+05, 5.5103e+06]])\n"
     ]
    }
   ],
   "source": [
    "## 按照行计算累计乘\n",
    "print(torch.cumprod(B,dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "7a469af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25.0108)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 标准差\n",
    "torch.std(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7d2e38",
   "metadata": {},
   "source": [
    "### PyTorch中的自动微分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d860ebd6",
   "metadata": {},
   "source": [
    "在torch中的torch.autograd模块，提供了实现任意标量值函数自动求导的类和函数。针对一个张量只需要设置参数requires_grad=True，通过相关计算即可输出其在传播过程中的梯度（导数）信息。\n",
    "\n",
    "下面使用一个实例来解释PyTorch中自动微分的计算，在PyTorch中生成一个矩阵张量x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "eade1b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.requires_grad: True\n",
      "y.requires_grad: True\n",
      "x: tensor([[1., 2.],\n",
      "        [3., 4.]], requires_grad=True)\n",
      "y: tensor(54., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1.0,2.0],[3.0,4.0]],requires_grad=True)\n",
    "## 默认requieres_grad=False\n",
    "y = torch.sum(x**2+2*x+1)\n",
    "print(\"x.requires_grad:\",x.requires_grad)\n",
    "print(\"y.requires_grad:\",y.requires_grad)\n",
    "print(\"x:\",x)\n",
    "print(\"y:\",y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe914f2",
   "metadata": {},
   "source": [
    "下面通过y.backward()来计算y在x的每个元素是上的导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "36ebbd3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.,  6.],\n",
       "        [ 8., 10.]])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 计算y在x上的梯度\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602d991d",
   "metadata": {},
   "source": [
    "在上面的程序中通过y.backward(0)即可自动计算出y在x的每个元素是上的导数，然后通过x的grad属性即可获取此时x的梯度信息，计算得到梯度值为2x+2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ea933",
   "metadata": {},
   "source": [
    "### torch.nn模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ab1d1d",
   "metadata": {},
   "source": [
    "torch.nn模块包含torch已经准备好的层，方便使用者调用构建网络。下面介绍卷积层、池化层、激活函数层、循环层、全连接层的相关使用方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aafc5d2",
   "metadata": {},
   "source": [
    "### 卷积层\n",
    "卷积可以看做是输入和卷积核之间的内积运算，是两个实值函数之间的一种数学运算。\n",
    "\n",
    "在卷积运算中，通常使用卷积核将输入数据进行卷积运算得到输出作为特征映射。\n",
    "\n",
    "使用卷积运算在图像领域的应用有三个好处，即卷积稀疏连接、参数共享、等变表示。\n",
    "\n",
    "1. 在卷积神经网络中，通过输入卷积核来进行卷积操作，使输入单元（图像或特征映射）和输出单元（特征映射）之间的连接是稀疏的，从而减少需要训练的参数数量，加快网络的计算速度。\n",
    "\n",
    "2. 卷积操作的参数共享特点主要体现在同一组参数可以被多个函数或操作共同使用。在卷积神经网络中，针对不同的输入会利用同样的卷积核来获得相应的输出。这样可以使得只需要训练一个参数集，而不需要对每个位置学习一个参数集合。\n",
    "\n",
    "3. 由于卷积核尺寸可以远小于输入尺寸，减少需要学习的参数的数量，并且针对每个卷积层可以使用多个卷积核获取输入的特征映射，对数据具有很强的特征提取和表示能力，并且在卷积运算之后，使得卷积神经网络对输入的图像具有平移不变的性质。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72dff26",
   "metadata": {},
   "source": [
    "在PyTorch中针对卷积操作的对象和使用场景不同，有一维卷积、二维卷积、三维卷积、转置卷积（卷积操作的逆操作），他们都可以从torch.nn模块中调用。\n",
    "\n",
    "常用的卷积操作所对应的类\n",
    "1. 一维卷积：torch.nn.Conv1d()\n",
    "2. 一维转置卷积：torch.nn.ConvTranspose1d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "ed142eca",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'in_channels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0b/gzxsb2qj1570ktpd81lr27zw0000gn/T/ipykernel_22276/3552507727.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## 以torch.nnConv2d()为例，介绍卷积在图像上的使用方法\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m torch.nn.Conv2d(in_channels,\n\u001b[0m\u001b[1;32m      3\u001b[0m                \u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'in_channels' is not defined"
     ]
    }
   ],
   "source": [
    "## 以torch.nnConv2d()为例，介绍卷积在图像上的使用方法\n",
    "torch.nn.Conv2d(in_channels,\n",
    "               out_channels,\n",
    "               kernel_size,\n",
    "               stride=1,\n",
    "               padding=0,\n",
    "               dilation=1,\n",
    "               groups=1,\n",
    "               bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a300d66c",
   "metadata": {},
   "source": [
    "主要参数说明：\n",
    "in_channels:（整数）输入图像的通道数。\n",
    "\n",
    "out_channels:（整数）经过卷积运算后，输出特征映射的数量。\n",
    "\n",
    "kernel_size:（整数或数组）卷积核的大小\n",
    "\n",
    "stride:（整数或者数组，正数）卷积的步长，默认为1\n",
    "\n",
    "padding:（整数或者数组，正数）在输入两边进行0填充的数量，默认为0\n",
    "\n",
    "dilation:（整数或者数组，正数）卷积核元素之间的步幅，该参数可调整空洞卷积核的空洞大小，默认为1\n",
    "\n",
    "groups:（整数，正数）从输入通道到输出通道的阻塞连接数\n",
    "\n",
    "bias:（布尔值，正数）如果bias=True，则添加偏置，默认为True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40277037",
   "metadata": {},
   "source": [
    "下面使用一张图像来展示经过卷积后，输出的特征映射的结果。使用PIL包读取图像数据，使用matplotlib包来可视化图像和卷积后的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4115c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "## 读取图像->转化为灰度图片->转化为Numpy数组\n",
    "myim = Image.open(\"1.png\") ## 获取图像数据\n",
    "myimgray = np.array(myim.convert(\"L\"),dtype=np.float32) ## 转化为灰度图像\n",
    "## 可视化图片\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(myimgray,cmap=plt.cm.gray) ## 将图像可视化\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0d0431",
   "metadata": {},
   "source": [
    "经过上述操作的到一个512×512的数组，在使用PyTorch进行卷积操作之前，需要将其转化为1×1×512×512的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159eab81",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 将数组转化为张量\n",
    "imh,imw = myimgray.shape\n",
    "print(imh,imw)\n",
    "myimgray_t = torch.from_numpy(myimgray.reshape((1,1,imh,imw)))\n",
    "myimgray_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45acaf77",
   "metadata": {},
   "source": [
    "卷积时需要将图像转化为四维来表示[batch,channel,h,w]。在图像进行卷积操作之后，获得两个特征映射。第一个特征映射使用图像轮廓提取卷积核获取，第二个使用的卷积核为随机数，卷积核大小为5×5，对图像的边缘不使用0填充，所以卷积后输出特征映射的尺寸为508×508.\n",
    "\n",
    "使用下面的程序进行卷积运算，并对卷积后的两个特征映射进行可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968767f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 对灰度图像进行卷积提取图像轮廓\n",
    "kersize = 5 ## 定义边缘检测卷积核，并将维度处理为1*1*5*5\n",
    "ker = torch.ones(kersize,kersize,dtype=torch.float32)*-1\n",
    "ker[2,2] = 24\n",
    "ker = ker.reshape((1,1,kersize,kersize))\n",
    "## 进行卷积操作\n",
    "conv2d = nn.Conv2d(1,2,(kersize,kersize),bias = False)\n",
    "\n",
    "## 设置卷积时使用的核，第一个核使用边缘检测核\n",
    "conv2d.weight.data[0] = ker\n",
    "\n",
    "## 对灰度图像进行卷积操作\n",
    "imconv2dout = conv2d(myimgray_t)\n",
    "\n",
    "## 对卷积后的输出进行维度压缩\n",
    "imconv2dout_im = imconv2dout.data.squeeze()\n",
    "print(\"卷积后尺寸：\",imconv2dout_im.shape)\n",
    "\n",
    "## 可视化卷积后的图像\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(imconv2dout_im[0],cmap=plt.cm.gray)\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(imconv2dout_im[1],cmap=plt.cm.gray)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6976540",
   "metadata": {},
   "source": [
    "### 池化层\n",
    "池化操作的一个重要目的是进一步处理卷积后得到的特征（主要是降维），起到对数据进一步浓缩的效果，从而缓解计算时的压力。\n",
    "\n",
    "池化会选择一定大小的区域，并将该区域内的像素值使用一个代表元素表示。如果使用平均值，则称为平均值池化，如果使用最大值则称为最大值池化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05211983",
   "metadata": {},
   "outputs": [],
   "source": [
    "## torch.nn.MaxPool2d()池化操作相关参数的应用\n",
    "torch.nn.MaxPool2d(kernel_size,\n",
    "                  stride=None,\n",
    "                  padding=0,\n",
    "                  dilation=1,\n",
    "                  return_indices=False,\n",
    "                  ceil_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034f499a",
   "metadata": {},
   "source": [
    "kernel_size:最大值池化的窗口大小\n",
    "\n",
    "stride:最大值池化窗口移动的步长，默认值是kernel_size\n",
    "\n",
    "padding:输入的每一条边补充0的层数\n",
    "\n",
    "dilation:控制窗口中元素步幅的参数\n",
    "\n",
    "return_indices:如果为True，则会返回输出最大值的索引，便于之后的torch.nn.MaxUnpool2d操作\n",
    "\n",
    "ceil_mode:如果等于True，计算输出信号大小的时候，会使用向上取整，默认为向下取整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365dbd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 对卷积后的结果进行最大值池化\n",
    "maxpool2 = nn.MaxPool2d(2,stride=2)\n",
    "pool2_out = maxpool2(imconv2dout)\n",
    "pool2_out_im = pool2_out.squeeze()\n",
    "pool2_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7c2c8d",
   "metadata": {},
   "source": [
    "经过上面的程序发现，原始的特征映射经过窗口为2*2，步长为2的最大值池化后，特征映射的尺寸减少了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce04b848",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 可视化最大值池化后的结果\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(pool2_out_im[0].data,cmap=plt.cm.gray)\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(pool2_out_im[1].data,cmap=plt.cm.gray)\n",
    "plt.axis(\"off\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c28393",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 使用nn.AvgPool2d()函数，对卷积后的输出进行平均值池化\n",
    "avgpool2 = nn.AvgPool2d(2,stride=2)\n",
    "pool2_out = avgpool2(imconv2dout)\n",
    "pool2_out_im = pool2_out.squeeze()\n",
    "pool2_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799b7c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 可视化平均值池化后的结果\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(pool2_out_im[0].data,cmap=plt.cm.gray)\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(pool2_out_im[1].data,cmap=plt.cm.gray)\n",
    "plt.axis(\"off\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf12c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 对卷积后的结果进行自适应平均值池化\n",
    "AdaAvgpool2 = nn.AdaptiveAvgPool2d(output_size = (100,100))\n",
    "pool2_out = AdaAvgpool2(imconv2dout)\n",
    "pool2_outim=pool2_out.squeeze()\n",
    "pool2_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa3d9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 可视化自适应平均值池化后的结果\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(pool2_out_im[0].data,cmap=plt.cm.gray)\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(pool2_out_im[1].data,cmap=plt.cm.gray)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eda7d9f",
   "metadata": {},
   "source": [
    "从结果来看，池化后的特征映射尺寸变小，图像变得更加模糊"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803b0e13",
   "metadata": {},
   "source": [
    "### 激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28302ff3",
   "metadata": {},
   "source": [
    "常用的激活函数通常为S型(Sigmoid)激活函数、双曲正切(Tanh)激活函数、线性修正单元(ReLU)激活函数等。\n",
    "\n",
    "torch.nn.Sigmoid()对应的是Sigmoid激活函数，也叫logistic激活函数，计算方式为\n",
    "$f(x)=\\frac{1}{1+e^{-x}}$\n",
    "\n",
    "其值域为(0,1)。该函数在神经网络早期是很常用的激活函数，但是当输入远离坐标原点时，函数的梯度变得很小，几乎为零，所以会影响参数的更新速度。\n",
    "\n",
    "torch.nn.Tanh()对应双曲正切函数，计算公式为$f(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}$\n",
    "\n",
    "其值域是(-1,1)整个函数以0为中心，虽然Tanh函数和Sigmoid函数一样在输入很大或很小时，梯度很小，不利于权重更新，但由于Tanh的取值输出以0位对称，使用效果会比Sigmoid好很多。\n",
    "\n",
    "torch.nn.ReLU()对应ReLU函数，又称修正线性单元，计算方式为$f(x)=max(0,x)$\n",
    "\n",
    "ReLU函数只保留大于0的输出，其他输出则会设置为0。在输入正数的时候不存在梯度饱和的问题。其计算速度相比于其他类型激活函数要快很多，而且该激活函数只有线性关系，所以不关前向传播还是反向传播速度都很快。\n",
    "\n",
    "torch.nn.Softplus()对应平滑近似ReLU的激活函数，其计算公式为$f(x)=\\frac{1}{β}log(1+e^{βx})$\n",
    "\n",
    "β默认为1。该函数对任意位置都可以求导，而且尽可能地保留了ReLU激活函数的优点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "c1ddfe41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x216 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# 使用PyTorch中的激活函数可视化上面介绍的几种激活函数\n",
    "x = torch.linspace(-6,6,100)\n",
    "sigmoid = nn.Sigmoid() # Sigmoid激活函数\n",
    "ysigmoid = sigmoid(x)\n",
    "tanh = nn.Tanh() # Tanh激活函数\n",
    "ytanh = tanh(x)\n",
    "relu = nn.ReLU() # ReLU激活函数\n",
    "yrelu = relu(x)\n",
    "softplus = nn.Softplus() # Softplus激活函数\n",
    "ysoftplus = softplus(x)\n",
    "plt.figure(figsize=(14,3)) # 可视化激活函数\n",
    "plt.subplot(1,4,1)\n",
    "plt.plot(x.data.numpy(),ysigmoid.data.numpy(),\"r-\")\n",
    "plt.title(\"Sigmoid\")\n",
    "plt.grid()\n",
    "plt.subplot(1,4,2)\n",
    "plt.plot(x.data.numpy(),ytanh.data.numpy(),\"r-\")\n",
    "plt.title(\"Tanh\")\n",
    "plt.grid()\n",
    "plt.subplot(1,4,3)\n",
    "plt.plot(x.data.numpy(),yrelu.data.numpy(),\"r-\")\n",
    "plt.title(\"ReLU\")\n",
    "plt.grid()\n",
    "plt.subplot(1,4,4)\n",
    "plt.plot(x.data.numpy(),ysoftplus.data.numpy(),\"r-\")\n",
    "plt.title(\"Softplus\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63257465",
   "metadata": {},
   "source": [
    "### 循环层\n",
    "在PyTorch中，提供了三种循环层的实现：\n",
    "1. torch.nn.RNN() 多层RNN单元\n",
    "2. torch.nn.LSTM() 多层长短期记忆LSTM单元\n",
    "3. torch.nn.GRU() 多层门限循环GRU单元"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bcd15a",
   "metadata": {},
   "source": [
    "下面以torch.nn.RNN()为例，介绍循环层的参数、输入和输出：\n",
    "\n",
    "参数说明：\n",
    "input_size:输入x的特征数量\n",
    "\n",
    "hidden_size:隐层的特征数量\n",
    "\n",
    "num_layers:RNN网络的层数\n",
    "\n",
    "nonlinearity:指定非线性函数使用tanh还是relu，默认为tanh\n",
    "\n",
    "bias:如果是False,那么RNN层就不会使用偏置权重，默认为True\n",
    "\n",
    "batch_first:如果是True,那么输入和输出的shape应该是[batch_sie,time_step,feature]\n",
    "\n",
    "dropout:如果值非零，那么除了最后一层外，其他RNN层的输出都会套上一个dropout层，默认为0\n",
    "\n",
    "bidirectional:如果是True,将会变成一个双向RNN，默认为False\n",
    "\n",
    "RNN的输入为input和h_0,其中input是一个形状为(seq_len,batch,input_size)的张量。h_0则是一个形状为(num_layers*num_direcions,batch,hidden_size)保存着初始隐状态的张量，如果不提供就默认为0。如果是双向RNN，num_directions等于2，否则等于1.\n",
    "\n",
    "RNN的输出为output和h_n，其中：\n",
    "\n",
    "output是一个形状为(seq_len,batch,hidden_size*num_directions)的张量，保存着RNN最后一层的输出特征。如果输入是被填充过的序列，那么输出也是被填充过的序列。\n",
    "\n",
    "h_n是一个形状为(num_layers*num_directions,batch,hidden_size)的张量，保存着最后一个时刻的隐状态"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92617ed",
   "metadata": {},
   "source": [
    "### 全连接层\n",
    "通常说的全连接层是由多个神经元所组成的层，其所有的输出和该层的所有输入都有连接，即每一个输入都会影响所有神经元的输出。\n",
    "\n",
    "在PyTorch中的nn.Linear()表示线性变换，全连接层可以看作是nn.Linear()表示线性变换层再加上一个激活函数层所构成的结构。\n",
    "\n",
    "nn.Linear()全连接操作及相关参数如下：\n",
    "\n",
    "torch.nn.Linear(in_features,out_feartures,bias=True)\n",
    "\n",
    "参数说明：\n",
    "1. in_features:每个输入样本的特征数量\n",
    "2. out_features:每个输出样本的特征数量\n",
    "3. bias: 若设置为False，则该层不会学习偏置\n",
    "\n",
    "torch.nn.Linear()的输入为(N,in_features)的张量，输出为(N,out_features)的张量。\n",
    "\n",
    "全连接层的应用十分广泛，只有全连接层组成的网络是全连接神经网络，可用于数据的分类或回归预测。卷积神经网络和循环神经网络的末端通常会由多个全连接层组成。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9792e44a",
   "metadata": {},
   "source": [
    "## PyTorch中数据操作和预处理\n",
    "在PyTorch中torch.utils,data模块包含着一些常用的数据预处理的操作，主要用于数据的读取、切分、准备等。常用的函数如下表所示：\n",
    "\n",
    "| 类                               | 功能                                       |\n",
    "| -------------------------------- | ------------------------------------------ |\n",
    "| torch.utils.data.TensorDataset() | 将数据处理为张量                           |\n",
    "| torch.utils.data.ConcatDataset() | 连接多个数据集                             |\n",
    "| torch.utils.data.Subset()        | 根据索引获取数据集的子集                   |\n",
    "| torch.utils.data.DataLoader()    | 数据加载器                                 |\n",
    "| torch.utils.data.random_split()  | 随机将数据集拆分为给定长度的非重叠新数据集 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b00f6f",
   "metadata": {},
   "source": [
    "### 高维数组\n",
    "在很多情况下，我们需要从文本中读取高维数组数据，这类数据的特征是每个样本都有很多个预测变量（特征）和一个被预测变量（目标标签），特征通常是数值变量或离散变量。\n",
    "被预测变量如果是连续的数值，则对应着回归问题，如果是离散变量，则对应分类问题。在使用PyTorch建立模型对数据进行学习前，通常要对数据进行预处理，并将它们转化为网络需要的数据格式。\n",
    "\n",
    "为了展示全连接神经网络模型，下面使用sklearn中提供的数据集load——boston和load_iris来进行回归和分类的数据准备。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890ad959",
   "metadata": {},
   "source": [
    "1. 回归数据准备\n",
    "首先加载相应的模块，然后读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "1b92ce1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boston_X.dtype: float64\n",
      "train_yt.dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "from sklearn.datasets import load_boston,load_iris\n",
    "## 读取波士顿回归数据\n",
    "boston_X,boston_y = load_boston(return_X_y=True)\n",
    "print(\"boston_X.dtype:\",boston_X.dtype)\n",
    "print(\"train_yt.dtype:\",boston_y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6e2739",
   "metadata": {},
   "source": [
    "从上面程序输出可以发现数据集的特征和被预测变量都是Numpy的64位浮点型数据，而PyTorch需要数据为torch的32位浮点型张量，故需要进行数据格式转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "a3c1b098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_xt.dtype: torch.float32\n",
      "train_yt.dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "## 先将Numpy数据转化为32位浮点型，然后使用torch.from_numpy()函数，将数组转化为张量\n",
    "train_xt = torch.from_numpy(boston_X.astype(np.float32))\n",
    "train_yt = torch.from_numpy(boston_y.astype(np.float32))\n",
    "print(\"train_xt.dtype:\",train_xt.dtype)\n",
    "print(\"train_yt.dtype:\",train_yt.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d475b264",
   "metadata": {},
   "source": [
    "在训练全连接神经网络时，通常一次使用一个batch的数据进行权重更新，torch.utils.data.DataLoader()函数可以将输入的数据集（包含数据特征张量和被预测变量张量）获得一个加载器，每次迭代可使用一个batch的数据，其使用方法如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "110a2935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b_x.shape: torch.Size([64, 13])\n",
      "b_y.shape: torch.Size([64])\n",
      "b_x.dtype: torch.float32\n",
      "b_y,dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "## 使用TensorDataset将X和Y整理到一起\n",
    "train_data = Data.TensorDataset(train_xt,train_yt)\n",
    "\n",
    "## 定义一个数据加载器，将训练数据集进行批量处理\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset = train_data, ## 使用的数据集\n",
    "    batch_size = 64, ## 批处理样本大小\n",
    "    shuffle = True, ## 每次迭代前打乱数据\n",
    "    num_workers = 1, ## 使用两个进程\n",
    ")\n",
    "\n",
    "## 检查训练数据集的一个batch的样本的维度是否正确\n",
    "for step,(b_x,b_y) in enumerate(train_loader):\n",
    "    if step > 0:\n",
    "        break\n",
    "        \n",
    "## 输出训练图像的尺寸和标签的尺寸及数据类型\n",
    "print(\"b_x.shape:\",b_x.shape)\n",
    "print(\"b_y.shape:\",b_y.shape)\n",
    "print(\"b_x.dtype:\",b_x.dtype)\n",
    "print(\"b_y,dtype:\",b_y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143a1b70",
   "metadata": {},
   "source": [
    "在上面的数据中，每64个样本为一个batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f78a9d",
   "metadata": {},
   "source": [
    "2. 分类数据准备\n",
    "分类数据和回归数据的不同点在于，分类数据的被预测变量为离散型变量，所以在使用PyTorch定义的网络模型时，默认的预测标签是64位有符号整形数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "1dd771a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris_x.dtype: float64\n",
      "irisy: int64\n"
     ]
    }
   ],
   "source": [
    "## 处理分类数据\n",
    "iris_x,irisy = load_iris(return_X_y=True)\n",
    "print(\"iris_x.dtype:\",iris_x.dtype)\n",
    "print(\"irisy:\",irisy.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b0dfa8",
   "metadata": {},
   "source": [
    "从上面的输出可知，特征数据（X）为64位浮点型，标签（Y）为64位整形.而torch构建的网络中，X默认的数据格式为torch.float32,所以数据的特征X需要转化为32位浮点型，数据的类别标签Y要转化为64位有符号整形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "18c70d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_xt.dtype torch.float32\n",
      "train_yt.dtype torch.int64\n"
     ]
    }
   ],
   "source": [
    "## 训练集X和Y转化为张量\n",
    "train_xt = torch.from_numpy(iris_x.astype(np.float32))\n",
    "train_yt = torch.from_numpy(irisy.astype(np.int64))\n",
    "print(\"train_xt.dtype\",train_xt.dtype)\n",
    "print(\"train_yt.dtype\",train_yt.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e8b037",
   "metadata": {},
   "source": [
    "准备好数据后，再使用Data.TensorDataset()和Data.DataLoader()定义数据加载器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "c9f1252d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b_x.shape: torch.Size([10, 13])\n",
      "b_y.shape: torch.Size([10])\n",
      "b_x.dtype: torch.float32\n",
      "b_y,dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "## 使用TensorDataset 将X和Y整理到一起\n",
    "tarain_data = Data.TensorDataset(train_xt,train_yt)\n",
    "## 定义一个数据加载器，将训练数据进行批量处理\n",
    "train_loader = Data.DataLoader(dataset = train_data,\n",
    "                              batch_size = 10,\n",
    "                              shuffle = True,\n",
    "                              num_workers = 1,\n",
    "                              )\n",
    "## 检查训练数据集中的一个batch样本的维度是否正确\n",
    "for step,(b_x,b_y) in enumerate(train_loader):\n",
    "    if step > 0:\n",
    "        break\n",
    "\n",
    "## 输出训练图像的尺寸和标签的尺寸与数据类型\n",
    "print(\"b_x.shape:\",b_x.shape)\n",
    "print(\"b_y.shape:\",b_y.shape)\n",
    "print(\"b_x.dtype:\",b_x.dtype)\n",
    "print(\"b_y,dtype:\",b_y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3ff84c",
   "metadata": {},
   "source": [
    "### 图像数据\n",
    "torchvision中的datasets模块包含多种常用的分类数据集下载及导入函数，可以很方便地导入数据以及验证所建立的模型效果\n",
    "\n",
    "torchvision中的transformers模块可以针对每张图像进行预处理操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aaf04f",
   "metadata": {},
   "source": [
    "下面代码以实际的数据集为例，结合torchvision中的相关模块的使用，展示图像数据的预处理操作。\n",
    "一种是从torchvision中的datasets模块中导入数据、另一种是从文件夹中导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "fc7ffb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "from torchvision.datasets import FashionMNIST\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e31b637",
   "metadata": {},
   "source": [
    "1. 从torchvision的datasets模块中导入数据并预处理\n",
    "以导入FasionMNIST数据集为例，该数据集包含一个60000张28*28的灰度图片作为训练集，以及10000张28*28的灰度图片作为测试机。数据共10类，分别是鞋子、T恤等服饰类图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "3acae799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938\n"
     ]
    }
   ],
   "source": [
    "## 使用FashionMNIST数据，准备训练数据集\n",
    "train_data = FashionMNIST(root = \"./data/FashionMNIST\", ## 数据路径\n",
    "                         train = True, ## 只使用训练集\n",
    "                         transform = transforms.ToTensor(),\n",
    "                         download=False ## 没下载过则填True\n",
    "                         )\n",
    "## 定义一个数据加载器\n",
    "train_loader = Data.DataLoader(dataset = train_data,\n",
    "                              batch_size = 64,\n",
    "                              shuffle = True,\n",
    "                              num_workers = 2,\n",
    "                              )\n",
    "\n",
    "## 计算train_loader 有多少个batch\n",
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a60553",
   "metadata": {},
   "source": [
    "1. 参数transform用于指定数据集的变换，transform = transforms.ToTensor()表示将数据中的像素值转换到0~1之间，并且将图像数据从形状为[H,W,C]转换成形状为[C,H,W].\n",
    "2. 在数据导入后需要利用数据加载器DataLoader()将整个数据集切分为多个batch，用于网络优化时利用梯度下降算法进行求解。\n",
    "\n",
    "对训练数据进行处理后，可以使用相同的方法对测试集进行处理，也可以使用如下方式对测试集进行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "2bb54dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data_x.shape: torch.Size([10000, 1, 28, 28])\n",
      "test_data_y.shape: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "## 对测试集进行处理\n",
    "test_data =  FashionMNIST(\n",
    "    root = \"./data/FashionMNIST\",\n",
    "    train = False,\n",
    "    download = True\n",
    ")\n",
    "\n",
    "## 为数据添加一个通道维度，并且取指范围缩放到0~1之间\n",
    "test_data_x = test_data.data.type(torch.FloatTensor) / 255.0\n",
    "test_data_x = torch.unsqueeze(test_data_x,dim = 1)\n",
    "test_data_y = test_data.targets ## 测试集的标签\n",
    "print(\"test_data_x.shape:\", test_data_x.shape)\n",
    "print(\"test_data_y.shape:\", test_data_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f3d66f",
   "metadata": {},
   "source": [
    "上述程序将数据集中的像素值除以255，使像素值转化到0~1之间，再使用torch.unsqueeze()为数据添加一个通道，即可得到测试数据集。在test_data中使用test_data.data获取图像数据，使用test_data.targets获取每个图像所对应的标签"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e24acb1",
   "metadata": {},
   "source": [
    "### 文本数据\n",
    "对文本数据进行分类时深度学习任务中常见的应用，但是PyTorch建立的深度学习网络不能直接作用于文本数据，需要对文本数据进行相应的预处理。具体操作如下：\n",
    "\n",
    "在指定的文件夹中包含两个文本数据的数据集，每个数据集中有两列数据，分别是表示文本对应的标签变量label和表示文本的内容变量text，针对这种数据类型，可以非常方便地利用torchtext库中的函数进行预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "f67da22f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/textdata/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0b/gzxsb2qj1570ktpd81lr27zw0000gn/T/ipykernel_22276/2572110136.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m## 读取数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m traindata,testdata = data.TabularDataset.splits(path=\"data/textdata\",format=\"csv\",\n\u001b[0m\u001b[1;32m     28\u001b[0m                                                \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_data_fields\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                                                \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"test.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torchtext/legacy/data/dataset.py\u001b[0m in \u001b[0;36msplits\u001b[0;34m(cls, path, root, train, validation, test, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         train_data = None if train is None else cls(\n\u001b[0m\u001b[1;32m     78\u001b[0m             os.path.join(path, train), **kwargs)\n\u001b[1;32m     79\u001b[0m         val_data = None if validation is None else cls(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torchtext/legacy/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, format, fields, skip_header, csv_reader_params, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m             'tsv': Example.fromCSV, 'csv': Example.fromCSV}[format]\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'csv'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0municode_csv_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcsv_reader_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/textdata/train.csv'"
     ]
    }
   ],
   "source": [
    "from torchtext.legacy import data\n",
    "\n",
    "## 定义文本切分方法，使用空格切分即可\n",
    "mytokenize = lambda x: x.split()\n",
    "\n",
    "## 定义将文本转化为张量的相关操作\n",
    "TEXT = data.Field(sequential=True, ## 表明输入的文本是字符\n",
    "                 tokenize=mytokenize, ## 使用自定义分词方法\n",
    "                 use_vocab=True, ## 创建一个词汇表\n",
    "                 batch_first=True, ## batch优先的数据方式\n",
    "                 fix_length=200 ## 每个句子固定长度为200\n",
    "                 )\n",
    "## 定义将标签转化为张量的相关操作\n",
    "LABEL = data.Field(sequential=False,\n",
    "                  use_vocab=False,\n",
    "                   pad_token=None,\n",
    "                   unk_token=None\n",
    "                  )\n",
    "\n",
    "## 对所要读取的数据集的每列进行处理\n",
    "text_data_fields = [\n",
    "    (\"label\", LABEL), ## 对标签的操作\n",
    "    (\"text\", TEXT) ## 对文本的操作\n",
    "]\n",
    "\n",
    "## 读取数据\n",
    "traindata,testdata = data.TabularDataset.splits(path=\"data/textdata\",format=\"csv\",\n",
    "                                               train=\"train.csv\", fields=text_data_fields,\n",
    "                                               test = \"test.csv\", skip_header=True\n",
    "                                               )\n",
    "len(traindata),len(testdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204c6266",
   "metadata": {},
   "source": [
    "什么的程序主要进行了下面几个步骤：\n",
    "1. 首先从torchtext库导入data模块，接着利用lambda函数定义一个使用空格切分文本的切分函数，该函数可以将读入的文本利用空格切分为一个个单词\n",
    "2. 使用data.Field()函数类，分别定义将文本和标签转化为张量的相应操作，其可以通过张量来表示常见的文本。然后通过列表的方式将TEXT和LABEL与需要读取文件中的列明text和label相对应，最后得到text_data_field\n",
    "3. 通过data.TabularDataset.splits()函数从文件夹中读取指定的训练数据和测试数据，并返回相应的traindata和testdata.在读取文件时，通过fields = text_data_fields参数来确定对不同党的列进行不同的操作\n",
    "\n",
    "进行上述操作后，使用data.BucketIterator()函数将训练数据集和测试数据集定义为数据加载器\n",
    "\n",
    "在使用data.BucketIterator()前，需要对数据建立一个单词表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "b1fc52bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TEXT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0b/gzxsb2qj1570ktpd81lr27zw0000gn/T/ipykernel_22276/3482162880.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## 使用训练集构建单词表，并不指定预训练好的词向量\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mTEXT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraindata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m## 将训练集定义为数据加载器，便于对模型进行优化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBucketIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraindata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TEXT' is not defined"
     ]
    }
   ],
   "source": [
    "## 使用训练集构建单词表，并不指定预训练好的词向量\n",
    "TEXT.build_vocab(traindata,max_size=1000,vectors = None)\n",
    "\n",
    "## 将训练集定义为数据加载器，便于对模型进行优化\n",
    "train_iter = data.BucketIterator(traindata,batch_size=4)\n",
    "test_iter = data.BucketIterator(testdata,batch_size=4)\n",
    "for step,batch in enumerate(train_iter):\n",
    "    if step > 0:\n",
    "        break\n",
    "\n",
    "## 针对一个batch的数据，可以使用batch.label获得数据的类别标签\n",
    "print(\"数据的类别标签:\",batch.label())\n",
    "\n",
    "## batch.text是文本对应的编码向量\n",
    "print(\"数据的尺寸：\",batch.text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b6ac80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
